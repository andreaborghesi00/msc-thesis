\chapter{Methodology}
\section{Object Detection Architectures and Backbones}
\label{sec:arch_and_backbones}
The core of our lung nodule detection pipeline is built upon a robust and well-established object detection framework. For this research, experiments will be conducted on the architectures discussed in the previous chapter: Faster R-CNN and RetinaNet.

While these architectures defines the overall detection process, the quality of the features extracted from the input image is can impact its success. This feature extraction is performed by a deep convolutional neural network, commonly referred to as the backbone. The choice of backbone directly influences the model's performance, computational complexity, and memory footprint. A central component of our methodology is therefore to systematically evaluate the impact of different backbone architectures on the lung nodule detection task.

To investigate the trade-off between model performance and computational efficiency, we conducted experiments with three distinct backbone architectures, each representing a different design philosophy:

\begin{itemize}
    \item \textbf{ResNet50:} A canonical and widely adopted architecture, ResNet50 serves as a robust and powerful baseline. Its use of residual connections allows for the effective training of very deep networks, making it a standard choice for a vast range of computer vision tasks. With 25.5 million parameters, it represents a high-performance, but computationally intensive, option.

    \item \textbf{MobileNetV2:} Designed specifically for efficiency and deployment on resource constrained devices, MobileNetV2 is a lightweight architecture with only 5.4 million parameters. It utilizes inverted residual blocks and linear bottlenecks to achieve a favorable balance between accuracy and computational cost, making it an ideal candidate for exploring the feasibility of more accessible clinical tools.

    \item \textbf{EfficientNetV2-S:} This model represents a modern approach to network design, seeking to optimally balance accuracy, training speed, and parameter efficiency. By systematically scaling network depth, width, and resolution, and employing novel architectural blocks, EfficientNetV2-S (with 21.4 million parameters) offers performance competitive with larger models while maintaining a smaller computational footprint.
\end{itemize}

In all experiments, each backbone was initialized with weights pre-trained on the ImageNet1k dataset. This transfer learning approach is a standard practice that leverages the rich, hierarchical features learned from a large-scale, general-purpose dataset to significantly improve performance and reduce training time on smaller, specialized datasets such as the one used in this study. 

\subsection{Loss functions}
\label{sec:loss_functions}
A central metric for bounding box quality is the \textit{Intersection over Union} (IoU):
$$
\mathrm{IoU}(b, b^\ast) = \frac{\mathrm{area}(b \cap b^\ast)}{\mathrm{area}(b \cup b^\ast)},
$$
which measures the degree of spatial overlap between a predicted box $b$ and a ground-truth box $b^\ast$.
Variants such as Generalized IoU (GIoU), Distance IoU (DIoU), and Complete IoU (CIoU) incorporate additional geometric terms to improve optimization stability, particularly when boxes do not overlap \cite{rezatofighi2019giou,zheng2019diou, zheng2021ciou}.


Bounding box regression is also commonly evaluated in coordinate space using Minkowski distances.
The general $p$-norm between two vectors $u,v \in \mathbb{R}^n$ is defined as:
$$
\| u - v \|_p = \left( \sum_{j=1}^n |u_j - v_j|^p \right)^{\frac{1}{p}}.
$$
Special cases include the Manhattan distance ($L_1$ norm, $p=1$):
$$
\| u - v \|_1 = \sum_{j=1}^n |u_j - v_j|,
$$
and the Euclidean distance ($L_2$ norm, $p=2$):
$$
\| u - v \|_2 = \sqrt{\sum_{j=1}^n (u_j - v_j)^2}.
$$

In practice, modern detectors often adopt the Smooth-$L_1$ loss \cite{girshick2015fastrcnn} for bounding box regression, which combines the robustness of $L_1$ for large errors with the stability of $L_2$ for small errors:
$$
\mathrm{SmoothL1}(x) =
\begin{cases}
0.5\,x^2, & \text{if } |x| < 1, \\
|x| - 0.5, & \text{otherwise}.
\end{cases}
$$
This formulation reduces sensitivity to outliers while maintaining differentiability at the origin, making it suitable for gradient-based optimization.

As for the classification component, Faster R-CNN employs the standard cross-entropy loss for binary classification tasks, while RetinaNet utilizes the Focal Loss \cite{lin2018focalloss} to address class imbalance by down-weighting easy negatives and focusing training on hard examples.

\section{Data Preprocessing}
\subsection{Train-time Augmentations}
\label{sec:augmentation}
To further enhance the robustness of our models and improve their generalization capabilities, we apply a series of data augmentations during training. These augmentations include random rotations, flips, and intensity variations, which help to simulate the variability encountered in real-world clinical settings. By exposing the model to a diverse set of augmented images, we aim to reduce overfitting and improve the model's ability to generalize to unseen data. These augmentations are applied on-the-fly during training, ensuring that the model sees a different version of the data in each epoch.

The specific augmentations applied are as follows:
\begin{itemize}
    \item \textbf{Horizonal Flips}: Each image has a 50\% chance of being flipped horizontally. This augmentation helps the model learn invariance to left-right orientation, which is particularly relevant in medical imaging where anatomical structures can appear in various orientations.
    \item \textbf{Shift, Scale and Rotate}: Each image has a 50\% change of being randomly shifted (up to 5\% of image dimensions), scaled (between 0.9 and 1.1 times the original size), and rotated (up to 15 degrees). These transformations simulate variations in patient positioning and imaging angles, enhancing the model's ability to recognize nodules under different conditions.
    \item \textbf{Random Brightness and Contrast}: Each image has a 50\% chance of having its brightness and contrast randomly adjusted. Brightness is modified by a factor between -0.1 and +0.1, while contrast is adjusted by a factor between 0.9 and 1.1. This augmentation accounts for differences in imaging protocols and equipment, helping the model to generalize across varying image qualities.
    \item \textbf{CLAHE (Contrast Limited Adaptive Histogram Equalization)}: \\Each image has a 50\% chance of being processed with CLAHE. This technique enhances local contrast and improves the visibility of structures in areas that are too dark or too bright, which is particularly useful in medical images where subtle features can be critical for diagnosis \cite{mishra2021clahe}.
\end{itemize}


\section{Object Detection Pipeline}
To synthesize the previously described components, this section outlines the complete, end-to-end workflow used for training and evaluating the lung nodule detection models. This systematic process ensures reproducibility and provides a clear framework for the experiments presented in this thesis. The workflow proceeds in three main stages:

\begin{enumerate}
    \item \textbf{Data Preparation.} The initial raw 3D CT volumes from the DLCS dataset are processed through a sequential pipeline to generate the final training data:
    \begin{itemize}
        \item Each CT volume is first resampled to a uniform voxel spacing (Section \ref{sec:resampling}).
        \item 2D axial slices are extracted from each 3D volumetric annotation.
        \item The resulting set of slices is filtered using the Informed HU Thresholding strategy to reduce annotation noise (Section \ref{sec:dataset_pruning}).
        \item Finally, the 2.5D three-channel input representation is constructed for each target slice (Section \ref{sec:2.5d_approach}).
    \end{itemize}

    \item \textbf{Model Training.} The Faster R-CNN/RetinaNet model is trained on the prepared dataset. 
    \begin{itemize}
        \item The impact of different backbone architectures (ResNet50, MobileNetV2, EfficientNetV2-S) is systematically evaluated (Section \ref{sec:arch_and_backbones}).
        \item The training process incorporates extensive online data augmentation (Section \ref{sec:augmentation}) to prevent overfitting and is guided by a consistent set of hyperparameters (AdamW optimizer, Cosine Annealing scheduler).
    \end{itemize}

    \item \textbf{Evaluation.} The trained models are evaluated on a held-out test set. 
    \begin{itemize}
        \item The models' predictions are compared against the ground truth annotations.
        \item Performance is quantitatively measured using the standard COCO metrics of Mean Average Precision (mAP) and Mean Average Recall (mAR), as formally defined in Section \ref{sec:eval_metrics}.
    \end{itemize}
\end{enumerate}


\section{Classification Head Extension}
Discuss the classification network, how we extracted a classification dataset from the detection dataset using the bounding boxes. 


\section{Explainability Methods}
Describe how are we using the adapted CAM methods, which layers we're targeting and an example of expected output.
Explain how we're going to compare the explainability methods using the segmentation game.