\chapter{Introduction}
\label{ch:introduction}

\section{Problem Statement and Motivation}

Lung cancer remains one of the leading causes of cancer-related mortality worldwide, with early detection being crucial for improving patient outcomes and survival rates. Computed Tomography (CT) screening programs, such as the National Lung Screening Trial (NLST), have demonstrated the potential to reduce lung cancer mortality through early identification of pulmonary nodules \cite{aberle2011reduced, de2020reduced}. However, the manual interpretation of CT scans is a time-intensive process that places significant burden on radiologists, while also being subject to inter-observer variability and potential oversight of small or subtle lesions.

Radiologists are often required to review hundreds of slices per patient, sometimes across dozens of patients in a single day. Under such conditions, cognitive fatigue can accumulate, potentially leading to decreased diagnostic precision, delayed reading times, or even missed findings, especially for low-contrast or small nodules that may appear on only a few slices \cite{stec2018systematic, taylor2019fatigue}. This challenge is further amplified in high-volume screening programs, where maintaining consistent accuracy over long hours is difficult even for experienced professionals.

Artificial intelligence (AI) based detection tools have the potential to alleviate this strain by automatically flagging suspicious regions of interest, enabling radiologists to focus attention on the most relevant slices. Such systems do not replace human expertise but can act as a second reader, improving sensitivity to subtle findings, reducing oversight caused by fatigue, and providing explainable visual cues that support decision-making and increase trust in automated recommendations \cite{glikson2020human}. 
On this note, it is important to mention that it has been shown that the use of AI can instead increase fatigue to radiologists with higher workload if they have a low acceptance of AI \cite{liu2024artificial}. Regardless, the integration of AI tools into clinical workflows has been shown to enhance diagnostic accuracy  and efficiency, ultimately leading to better patient care and outcomes \cite{guermazi2022improving,huynh2020artificial}.

Nevertheless, existing AI solutions are often limited by computational demands and lack transparency in their predictions, motivating the need for efficient and explainable detection methods specifically tailored to lung nodule identification in CT scans.
This need is further reinforced by the recent European Union Artificial Intelligence Act, which establishes a regulatory framework that emphasizes transparency, accountability, and human oversight for AI systemsâ€”particularly in high-risk domains such as healthcare \cite{eu_ai_act}.

\section{Research Challenges}

Designing AI-based tools for lung nodule detection in CT scans involves navigating a complex landscape of technical and practical challenges that significantly limit the applicability of existing solutions in real-world clinical environments.

The most immediate challenge stems from the computational demands of state-of-the-art approaches. While three-dimensional convolutional neural networks can theoretically exploit the full volumetric context of CT scans to improve detection accuracy, their practical implementation reveals significant limitations. These models typically require substantial GPU memory, often exceeding 16GB, and demand extensive training times that can extend to days or weeks, even on high-end hardware such as A100 GPUs with 40GB of memory \cite{wu2018systematicanalysisstateoftheart3d}. While inference times for individual patient scans may be manageable, the development and iterative improvement of such models becomes prohibitively expensive and time-consuming. 
Such computational requirements create a fundamental mismatch with research development processes, where rapid experimentation and iteration are crucial for advancing the field, especially in small research groups with limited access to high-end hardware.
This computational constraint naturally leads to consideration of more efficient 2D approaches, which process individual CT slices rather than entire volumes. This approach significantly reduces memory requirements and training times, allowing for more agile development cycles, at the cost of losing correlations between slices that may be crucial for accurate nodule detection. Regardless, a 2D approach allows for better accessbility to healthcare professionals, as it can be run on standard laptops or workstations without the need for high-end GPUs.
Equally critical, yet often overlooked in the technical literature, is the challenge of explainability in object detection systems. The medical domain presents unique requirements for AI interpretability, driven by regulatory demands, clinical decision-making needs, and the fundamental requirement for physician trust and acceptance. However, most existing explainability methods were developed for image classification tasks \cite{chattopadhay_2018gradcam++,draelos2021hirescam,jiang2021layercam, selvaraju2019gradcam}, where the goal is to explain a single prediction score for an entire image. Object detection fundamentally differs in that as it produces structured outputs consisting of multiple bounding boxes, each with associated class probabilities and confidence scores.
This structural difference creates significant technical obstacles for applying standard explainability techniques.

\section{Research Questions and Objectives}
The challenges outlined in the previous section give rise to three primary research questions that guide this thesis:

\paragraph{RQ1: How can 2D object detection approaches achieve clinically relevant performance for lung nodule detection within computational constraints?}
This question addresses the fundamental trade-off between computational efficiency and detection performance. While 3D approaches theoretically offer superior performance by exploiting volumetric context, their computational demands make them impractical for many research and deployment scenarios. We investigate whether 2D slice-based detection can achieve acceptable performance levels while operating within the memory and processing constraints of standard hardware configurations.

\paragraph{RQ2: How can explainability techniques be adapted for the structured outputs of object detection models?}
Standard explainability techniques are designed for classification tasks with single prediction scores, but object detection produces complex structured outputs with multiple bounding boxes, confidence scores, and non-differentiable post-processing steps. This question explores how gradient-free Class Activation Map (CAM) methods can be modified to generate meaningful explanations for object detection predictions, addressing the technical challenges posed by structured outputs and non-differentiable operations.\bigskip

To address these research questions, this thesis pursues the following specific objectives:
\paragraph{O1: Develop a computationally efficient 2D object detection pipeline for lung nodule detection}
Implement and compare multiple object detection architectures and their variants.
Design preprocessing pipelines optimized for 2D slice-based analysis, including slice selection algorithms to maximize information content.
Achieve detection performance that demonstrates clinical viability while operating within 16GB GPU memory constraints.

\paragraph{O2: Adapt CAM methods for object detection tasks }

Research and adapt CAM methods that can handle structured object detection outputs.
Implement comparison algorithms that can assess similarity between structured predictions for explainability evaluation.
Implement end-to-end explainable object detection pipeline that provides both predictions and corresponding explanation maps.

\paragraph{O3: Demonstrate integration of detection and classification with explainability}

Extend the object detection pipeline with a classification head for detected regions.
Apply explainability methods to the detection components.

\paragraph{O4: Provide comprehensive experimental validation}
Evaluate the complete system on the established medical imaging datasets NLST \cite{nlst_data}, DLCSD24 \cite{dlcsd24, tushar2025ailunghealthbenchmarking}. 
% Compare performance against relevant baselines and alternative approaches
% Analyze computational efficiency and practical deployment considerations

\section{Thesis Structure}
\label{sec:thesis_structure}

This thesis is organized into six chapters, each building upon the last to present a comprehensive account of the research undertaken. The structure is designed to guide the reader from the foundational concepts to the final conclusions in a logical and coherent manner.

\paragraph{Chapter 2: Background} provides the necessary theoretical foundation for this work. It begins with a detailed overview of the fundamentals of object detection, discussing the distinctions between one-stage and two-stage detectors, as well as anchor-based and anchor-free methods. It then delves into the specifics of the RetinaNet and Faster R-CNN architectures, and formally defines the standard evaluation metrics of Average Precision and Average Recall. The second half of the chapter introduces the field of Explainable AI (XAI) for computer vision, focusing on Class Activation Maps (CAMs) and critically analyzing their inherent limitations when applied to object detection, thereby motivating the adaptations developed in this thesis.

\paragraph{Chapter 3: Data and Preprocessing} details the datasets and the extensive data preparation pipeline that form the empirical basis of this research. It introduces the two primary datasets used, the National Lung Screening Trial (NLST) and the Duke Lung Cancer Screening Dataset 2024 (DLCSD24). The chapter provides a step-by-step description of the preprocessing workflow, including the slice selection and pruning algorithms designed to handle annotation noise. This chapter also details the design of the various ablation studies, including those for dataset pruning and the 2.5D input representation. Finally, it describes the methodology for deriving the specialized dataset used for the subsequent nodule classification task.


\paragraph{Chapter 4: Methodology} presents the core experimental design of this research. It outlines the object detection and classification architectures, the choice of backbones, and the specific training procedures and hyperparameters. Furthermore, it provides a comprehensive description of the explainability methods, including the implementation details of the adapted CAM techniques and the design of the ``Inverse Distance Game" used for their quantitative evaluation.

\paragraph{Chapter 5: Results} presents the empirical findings of the experiments described in the previous chapter. This chapter is divided into three main parts: an analysis of the object detection performance; a quantitative evaluation of the explainability methods based on the Inverse Distance Game; and an analysis of the classification performance on the benign versus malignant nodule task.

\paragraph{Chapter 6: Conclusions} concludes the thesis by summarizing the key contributions and findings. It revisits the research questions posed in this introduction to discuss how they were addressed. The chapter also acknowledges the limitations of the study and, based on these, proposes several directions for future research.