\chapter{Introduction}
\label{ch:introduction}

\section{Problem Statement and Motivation}

Lung cancer remains one of the leading causes of cancer-related mortality worldwide, with early detection being crucial for improving patient outcomes and survival rates. Computed Tomography (CT) screening programs, such as the National Lung Screening Trial (NLST), have demonstrated the potential to reduce lung cancer mortality through early identification of pulmonary nodules \cite{aberle2011reduced, de2020reduced}. However, the manual interpretation of CT scans is a time-intensive process that places significant burden on radiologists, while also being subject to inter-observer variability and potential oversight of small or subtle lesions.

Radiologists are often required to review hundreds of slices per patient, sometimes across dozens of patients in a single day. Under such conditions, cognitive fatigue can accumulate, potentially leading to decreased diagnostic precision, delayed reading times, or even missed findings, especially for low-contrast or small nodules that may appear on only a few slices \cite{stec2018systematic, taylor2019fatigue}. This challenge is further amplified in high-volume screening programs, where maintaining consistent accuracy over long hours is difficult even for experienced professionals.

Artificial intelligence (AI) based detection tools have the potential to alleviate this strain by automatically flagging suspicious regions of interest, enabling radiologists to focus attention on the most relevant slices. Such systems do not replace human expertise but can act as a second reader, improving sensitivity to subtle findings, reducing oversight caused by fatigue, and providing explainable visual cues that support decision-making and increase trust in automated recommendations \cite{glikson2020human}. 
On this note, it is important to mention that the use of AI in radiology is not always well-accepted, as it has been shown by Liu et al. \cite{liu2024artificial} that radiologists with a higher workload and lower AI-acceptance are more likely to experience burnout. Regardless, the integration of AI tools into clinical workflows has been shown to enhance diagnostic accuracy  and efficiency, ultimately leading to better patient care and outcomes \cite{guermazi2022improving,huynh2020artificial}

Nevertheless, existing AI solutions are often limited by computational demands and lack transparency in their predictions, motivating the need for efficient and explainable detection methods specifically tailored to lung nodule identification in CT scans.
This need is further reinforced by the recent European Union Artificial Intelligence Act, which establishes a regulatory framework that emphasizes transparency, accountability, and human oversight for AI systemsâ€”particularly in high-risk domains such as healthcare \cite{eu_ai_act}.

\section{Research Challenges}

Designing AI-based tools for lung nodule detection in CT scans involves navigating a complex landscape of technical and practical challenges that significantly limit the applicability of existing solutions in real-world clinical environments.

The most immediate challenge stems from the computational demands of state-of-the-art approaches. While three-dimensional convolutional neural networks can theoretically exploit the full volumetric context of CT scans to improve detection accuracy, their practical implementation reveals significant limitations. These models typically require substantial GPU memory, often exceeding 16GB, and demand extensive training times that can extend to days or weeks, even on high-end hardware such as A100 GPUs with 40GB of memory \cite{wu2018systematicanalysisstateoftheart3d}. While inference times for individual patient scans may be manageable, the development and iterative improvement of such models becomes prohibitively expensive and time-consuming. This computational burden became particularly acute in our research setting where access to high-end hardware was limited and unreliable. Working with a T4 GPU (16GB), the memory constraints made 3D approaches largely infeasible for our experiments. While we had occasional access to an A100 through shared research infrastructure, the instability of access, environment limitations, and restricted interaction modes (Jupyter-only) severely hindered our exploratory research process. A few preliminary experiments to understand model behavior and optimal hyperparameters consumed weeks of our available computational time, making the iterative development essential for novel research practically impossible. Such computational requirements create a fundamental mismatch with research development processes, where rapid experimentation and iteration are crucial for advancing the field.
This computational constraint naturally leads to consideration of more efficient 2D approaches, which process individual CT slices rather than entire volumes. This approach significantly reduces memory requirements and training times, allowing for more agile development cycles, at the cost of losing correlations between slices that may be crucial for accurate nodule detection. Regardless, a 2D approach allows for better accessbility to healthcare professionals, as it can be run on standard laptops or workstations without the need for high-end GPUs.

Equally critical, yet often overlooked in the technical literature, is the challenge of explainability in object detection systems. The medical domain presents unique requirements for AI interpretability, driven by regulatory demands, clinical decision-making needs, and the fundamental requirement for physician trust and acceptance. However, most existing explainability methods were developed for image classification tasks \cite{selvaraju2019gradcam,chattopadhay_2018gradcam++,draelos2021hirescam,jiang2021layercam}, where the goal is to explain a single prediction score for an entire image. Object detection fundamentally differs in that it produces structured outputs consisting of multiple bounding boxes, each with associated class probabilities and confidence scores.
This structural difference creates significant technical obstacles for applying standard explainability techniques.

\section{Research Questions and Objectives}

\section{Thesis Contributions}

\section{Thesis Structure}