\chapter{Results}
\label{chap:results}
\section{Object Detection Performance Analysis}
%Present the results of the object detectors, making sure that each setup is clearly defined: architecture, backbone, training hyperparameters, eventual pretraining, etc.
%For each setup we present the performance using AP and AR metrics, their inference times, and the GPU memory usage.

To quantitatively assess the performance of our models and the viability of the proposed training strategies, we evaluated each model configuration using the mean Average Precision (mAP) metric, a standard for object detection tasks. We report mAP at an Intersection over Union (IoU) threshold of 0.50 (mAP@.50), which is a common benchmark, as well as the average mAP over IoU thresholds from 0.50 to 0.95 in steps of 0.05 (mAP@.50:.95) to provide a more comprehensive assessment of localization accuracy. Additionally, we include mAP at a lower IoU of 0.10 (mAP@.10) to gauge the models' ability to detect nodules even with less precise bounding boxes.

\subsubsection{NLST Pretraining Results}
First, we present the results of the models trained on the NLST dataset, which served as a pretraining step before fine-tuning on the DLCS dataset.
As mentioned in the previous section, these models are pretrained on the ImageNet dataset, a training from scratch approach would have needed a much larger dataset to achieve comparable results, so we decided to use these weights as a starting point.
The results of the models trained on the NLST dataset are shown in Table \ref{tab:nlst-models}.

\input{tables/nlst-models.tex}

Surprisingly, the RetinaNet architecture is comparable and, although not statistically significantly better, it presents slightly higher AP metrics compared to Faster R-CNN with the same backbone.
We will see later that this trend is reversed when the models are trained on the DLCS dataset, and from this one might infere that such a difference is due to the difference in difficulty of the datasets, with NLST being easier to detect, we might say that a simpler architecture is sufficient to achieve good results, and that the more complex Faster R-CNN is not able to extract more information from the data.

If we were to compare the results of the other backbones, we can see that the EfficientNetV2S backbone performs the best overall, with the MobileNet backbone beign the second best regardless of its extremely low number of parameters.

This results are also aiming to show that although many architectures are shipped with ResNet50 as a backbone, it is not the best choice for every task, especially when there are available more efficient and effective backbones that already exploit residual connections and other techniques to improve performance.

\subsubsection{DLCS Direct Training Results}
Next, we evaluated the models on the more clinically realistic and challenging DLCS dataset. The models were again initialized with ImageNet1K weights and trained directly on DLCS. This experiment was designed to quantify the difficulty of detecting the smaller, more subtle nodules characteristic of this dataset without the aid of domain-specific pretraining. The results are shown in Table \ref{tab:dlcs-models-not-pretrained}.

\input{tables/dlcs-models-not-pretrained.tex}

As expected, the performance of the models on the DLCS dataset is significantly lower than on the NLST dataset, reflecting the increased difficulty. As previously announced, now the Faster R-CNN architecture outperforms RetinaNet with EfficientNetV2S and ResNet50 backbones, while the MobileNet still performs better on with the RetinaNet architecture, making it a good candidate for lower-end devices.
The best performing model is the Faster R-CNN with EfficientNetV2S backbone, achieving an overall mAP@50:95 of 0.423, that is still not enough to be clinically useful, although its map@10 of 0.745 shows how it might be used as a screening tool, and that it performs its performance lowers quickly as the IoU threshold increases, which is a common issue with object detection models.

\subsubsection{DLCS Pretrained Results}
Finally, we evaluated the models pretrained on the NLST dataset and then fine-tuned on the DLCS dataset. This approach aims to leverage the larger, more visually distinct nodules in NLST to provide a strong initial feature representation for the more challenging nodules in DLCS, similar to a shallow curriculm learning approach.
The results of the models pretrained on NLST and then fine-tuned on DLCS are shown in Table \ref{tab:dlcs-models-pretrained}.

\input{tables/dlcs-models-pretrained.tex}

Unfortunately, the results of this fine-tuning approach are slightly worse than the direct training on DLCS with the ImageNet weights. Although for the best performing model this difference is not statistically significant, it is still a disappointing result.
This might be due to the fact that the NLST dataset is not large enough. Which is a common issue when fine-tuning on a small dataset, as the model might forget useful general features learned from the ImageNet dataset. Despite this problem can be mitigated by using a low learning rate and by freezing some of the layers -- both of which were done in this case -- it is yet a possibility when the dataset is small compared to its complexity. 
On a different note we can see that in this experimental setup the Faster R-CNN performs bettern than RetinaNet on every backbone, with the EfficientNetV2S backbone still being the best performing one.


\section{Explainability Performance Analysis}
%Discuss the performance of the explainability methods, comparing them based on the segmentation game, performing some statistical analysis on the results.
