\chapter{Results}
\label{chap:results}
\section{Object Detection Performance Analysis}
%Present the results of the object detectors, making sure that each setup is clearly defined: architecture, backbone, training hyperparameters, eventual pretraining, etc.
%For each setup we present the performance using AP and AR metrics, their inference times, and the GPU memory usage.

Our experimental approach involved a two-stage strategy designed to address both the technical challenges of developing robust object detection models and the practical constraints of limited computational resources.

\subsection{Dataset Characgteristics and Usage Strategy}
The National Lung Screening Trial (NLST) dataset served as our initial development and pretraining dataset. NLST contains individual CT slices with annotations for malignant nodules, characterized by relatively large and visually evident nodules that are easier to detect. The individual slice format eliminated the need for complex 3D-to-2D conversion pipelines, allowing us to focus initially on core object detection methodology without the additional complexity of volume processing.
In contrast, the Duke Lung Cancer Screening (DLCS) dataset represents a more challenging and clinically realistic scenario. DLCS provides full CT volumes with annotations for both benign and malignant nodules, many of which are smaller and more subtle than those in NLST. The volumetric format required implementation of our complete preprocessing pipeline, including the slice selection and pruning algorithms developed to optimize 2D detection performance from 3D volumes discussed in Chapter \ref{chap:data-and-preprocessing}. This dataset was used for final model evaluation, providing a more rigorous test of our methods in a realistic clinical context.

To assess the optimal training approach for our challenging DLCS evaluation scenario, we designed an ablation study comparing two training strategies:
\begin{itemize}
    \item \textbf{NLST Pretraining}: Models were pretrained on the NLST dataset before being fine-tuned on the DLCS dataset. This approach leverages the larger, more visually distinct nodules in NLST to provide a strong initial feature representation.
    \item \textbf{Direct Training on DLCS}: Models were trained directly on the DLCS dataset from ImageNet-pretrained weights, without prior exposure to NLST. This strategy aims to adapt the model directly to the more challenging and clinically relevant nodules in DLCS.
\end{itemize}

This comparison allows us to evaluate whether the progressive difficulty approach provides significant benefits in terms of detection performance over direct training on the more complex DLCS dataset. 

The transfer learning strategy from NLST to DLCS tries to address the large domain gap between a CT dataset and ImageNet, where the latter is primarily focused on natural images. Nonetheless it is important to note that pretraining on ImageNet is often a standard practice in computer vision as it provides a strong initial feature representation, especially on the first layers of the network that tend to capture low-level features such as edges, textures, and basic shapes, all of which are easily transferable across different domains.

\subsection{Model Architectures and Training Details}
We mainly implemented two object detection architectures: Faster R-CNN and RetinaNet, both of which are widely used in the field. Each architecture was tested on several backbones, namely ResNet50, MobileNetV2, and EfficientNetV2S.
The experimental framework was built entirely from scratch, allowing us to customize each component of the object detection pipeline, including the backbone selection and anchor box sizes, along with the usual training hyperparameters such as learning rate and batch size.

The implementation of these two architectures was mainly provided by the \texttt{torchvision} library \cite{torchvision2016} . Unfortunately, it does not provide an implementation of these architectures with EfficientNetV2S as a backbone, so we had to implement it ourselves and attach it to an FPN head. The same applied for the MobileNetV2 backbone although for the RetinaNet architecture only. 

Each experiment used mixed precision training, which allows most computation to be performed using 16-bit floating-point (FP16) precision, while maintaining 32-bit floating-point (FP32) precision for critical operations that require numerical stability such as loss computation and gradient accumulation \cite{micikevicius2018mixedprecisiontraining}.
This approach significantly reduces GPU memory usage and accelerates training without compromising model accuracy. The implementation of mixed precision training was provided by the \texttt{torch.cuda.amp} module, which automatically manages the scaling of gradients and the conversion between FP16 and FP32 as needed. 

The following hyperparameters were kept constant for all experiments to ensure a fair comparison.
Due to the pretrained nature of every experiment, the learning rate was set to a low value of $0.0001$ to avoid catastrophic forgetting, and the batch size was set to $8$ to fit the GPU memory constraints. The training was performed for a maximum of $30$ epochs, with early stopping based on the validation metrics to prevent overfitting and cut down on training time with a patience of $10$ epochs. For the same reason the validation set was used to monitor the training process and decided to run a validation step every $3$ epochs.
The optimzer used was AdamW, which is a variant of the Adam optimizer that decouples weight decay from the optimization step, providing better generalization performance in many cases \cite{loshchilov2019decoupled, kingma2017adam}.

As for the scheduler used, we opted for a Cosine Annealing scheduler, which graduelly reduces the learning rate over the course of training, allowing for a more fine-tuned convergence towards the end of the training process. We also experimented with Cosine Annealing with Warm Restarts, which periodically resets the learning rate to a higher value, but it often led to failed training runs due to the mixed precision training induced instability, so we decided to stick with the simpler version.
Lastly, for the DLCS dataset, the 3-channels contain the previous, current, and next slice of the analyzed nodule, allowing the model to have a better context of the nodule's surroundings, effectively simulating a 3D input while still using a 2D object detection architecture.

The experiments were run using \texttt{parallel} to allow queuing multiple training runs on the same GPU, which is particularly useful for hyperparameter tuning and ablation studies. This approach allows us to efficiently utilize the available GPU and time resources.

\subsection{Experimental Results}
To quantitatively assess the performance of our models and the viability of the proposed training strategies, we evaluated each model configuration using the mean Average Precision (mAP) metric, a standard for object detection tasks. We report mAP at an Intersection over Union (IoU) threshold of 0.50 (mAP@.50), which is a common benchmark, as well as the average mAP over IoU thresholds from 0.50 to 0.95 in steps of 0.05 (mAP@.50:.95) to provide a more comprehensive assessment of localization accuracy. Additionally, we include mAP at a lower IoU of 0.10 (mAP@.10) to gauge the models' ability to detect nodules even with less precise bounding boxes.

\subsubsection{NLST Pretraining Results}
First, we present the results of the models trained on the NLST dataset, which served as a pretraining step before fine-tuning on the DLCS dataset.
As mentioned in the previous section, these models are pretrained on the ImageNet dataset, a training from scratch approach would have needed a much larger dataset to achieve comparable results, so we decided to use these weights as a starting point.
The results of the models trained on the NLST dataset are shown in Table \ref{tab:nlst-models}.

\input{tables/nlst-models.tex}

Surprisingly, the RetinaNet architecture is comparable and, although not statistically significantly better, it presents slightly higher AP metrics compared to Faster R-CNN with the same backbone.
We will see later that this trend is reversed when the models are trained on the DLCS dataset, and from this one might infere that such a difference is due to the difference in difficulty of the datasets, with NLST being easier to detect, we might say that a simpler architecture is sufficient to achieve good results, and that the more complex Faster R-CNN is not able to extract more information from the data.

If we were to compare the results of the other backbones, we can see that the EfficientNetV2S backbone performs the best overall, with the MobileNet backbone beign the second best regardless of its extremely low number of parameters.

This results are also aiming to show that although many architectures are shipped with ResNet50 as a backbone, it is not the best choice for every task, especially when there are available more efficient and effective backbones that already exploit residual connections and other techniques to improve performance.

\subsubsection{DLCS Direct Training Results}
Next, we evaluated the models on the more clinically realistic and challenging DLCS dataset. The models were again initialized with ImageNet1K weights and trained directly on DLCS. This experiment was designed to quantify the difficulty of detecting the smaller, more subtle nodules characteristic of this dataset without the aid of domain-specific pretraining. The results are shown in Table \ref{tab:dlcs-models-not-pretrained}.

\input{tables/dlcs-models-not-pretrained.tex}

As expected, the performance of the models on the DLCS dataset is significantly lower than on the NLST dataset, reflecting the increased difficulty. As previously announced, now the Faster R-CNN architecture outperforms RetinaNet with EfficientNetV2S and ResNet50 backbones, while the MobileNet still performs better on with the RetinaNet architecture, making it a good candidate for lower-end devices.
The best performing model is the Faster R-CNN with EfficientNetV2S backbone, achieving an overall mAP@50:95 of 0.423, that is still not enough to be clinically useful, although its map@10 of 0.745 shows how it might be used as a screening tool, and that it performs its performance lowers quickly as the IoU threshold increases, which is a common issue with object detection models.

\subsubsection{DLCS Pretrained Results}
Finally, we evaluated the models pretrained on the NLST dataset and then fine-tuned on the DLCS dataset. This approach aims to leverage the larger, more visually distinct nodules in NLST to provide a strong initial feature representation for the more challenging nodules in DLCS, similar to a shallow curriculm learning approach.
The results of the models pretrained on NLST and then fine-tuned on DLCS are shown in Table \ref{tab:dlcs-models-pretrained}.

\input{tables/dlcs-models-pretrained.tex}

Unfortunately, the results of this fine-tuning approach are slightly worse than the direct training on DLCS with the ImageNet weights. Although for the best performing model this difference is not statistically significant, it is still a disappointing result.
This might be due to the fact that the NLST dataset is not large enough. Which is a common issue when fine-tuning on a small dataset, as the model might forget useful general features learned from the ImageNet dataset. Despite this problem can be mitigated by using a low learning rate and by freezing some of the layers -- both of which were done in this case -- it is yet a possibility when the dataset is small compared to its complexity. 
On a different note we can see that in this experimental setup the Faster R-CNN performs bettern than RetinaNet on every backbone, with the EfficientNetV2S backbone still being the best performing one.


\section{Explainability Evaluation}
To evaluate the explainability methods, we 

%Discuss the performance of the explainability methods, comparing them based on the segmentation game, performing some statistical analysis on the results.
